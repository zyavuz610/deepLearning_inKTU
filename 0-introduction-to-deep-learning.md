# Introduction to Deep Learning

Although machine learning has been around for years, three new trends have led to widespread use of machine learning:
* **Large dataset**
* Powerful and efficient parallel computing achieved by **GPU computing**.
* New algorithms

As a result, new models and algorithms have been developed.

GPUs are 10-100 times more efficient than traditional CPUs. That's why data scientists use GPUs when processing big data.
The foundation of deep learning was laid when shallow artificial neural networks became **deeper neural networks** by increasing the number of layers.

## History
* **1950s** : the first ideas about artificial intelligence
  * Alan Turing (1952), "Can a machine think?"
  * John McCarthy - Dartmouth Conference (1956), The first usuage of the word "Artificial Intelligenge"
* **1957** : First break
  * Definition of 'Perceptron' (Frank Rosenblatt): a nerve cell (takes multiple inputs and produces an output)
* **1959** : The first Artificial Intelligence Lab. at MIT
* **1969** : First winter
  * Minsky and Papert showed that the XOR problem cannot be solved with a single layer network structure. It has been understood that new methods are needed. After this date, it was understood that ANN could not be used for non-linear systems and the investments to AI were stopped. Neural networks research has entered a period of stagnation. This was the first winter.
* **1986** : Second break
  * Back-propagation algorithm was suggested by Hinton for training multilayer perceptrons.
  * Interest in artificial neural networks has started again.
* **1995** : Second winter
  * Support Vector Machines (SVM) was suggested. Tue use of Traditional Neural Networks was declined. 
  * This was the second winter.
* **2012** : Third break
  * Processing power with GPU and huge amount of data became usuable for training deep neural networks. 
  * One of the first applications was AlexNet that had won the imagenet competition with an acceptible accuracy.

